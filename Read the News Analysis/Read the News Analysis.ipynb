{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055a36a5",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Newspapers and their online formats supply the public with the information we need to understand the events occurring in the world around us. From politics to sports, the news keeps us informed, in the loop, and ready to make decisions about how to act in a rapidly changing world.\n",
    "\n",
    "Given the vast amount of news articles in circulation, identifying and organizing articles by topic is a useful activity. This can help you sift through the enormous amount of information out there so you can find the news relevant to your interests, or even allow you to build a news recommendation engine!\n",
    "\n",
    "The News International is the largest English language newspaper in Pakistan, covering local and international news across a variety of sectors. A selection of articles from a [Kaggle Dataset of The News International articles](https://www.kaggle.com/datasets/asad1m9a9h6mood/news-articles) is provided in the workspace.\n",
    "\n",
    "In this project, I will use **term frequency-inverse document frequency (tf-idf)** to analyze each article’s content and uncover the terms that best describe each article, providing quick insight into each article’s topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac9f11",
   "metadata": {},
   "source": [
    "### Project Goal\n",
    "Analyze news documents using Tf-idf NLP supervised machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd02c6",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "Before a text can be processed by a NLP model, the text data needs to be pre-processed. Text data pre-processing is the process of cleaning and prepping the text data to be processed by NLP models.\n",
    "\n",
    "Cleaning and prepping tasks:\n",
    "- **Noise removal** is a text pre-processing step concerned with removing unnecessary formatting from our text.\n",
    "- **Tokenization** is a text pre-processing step devoted to breaking up text into smaller units (usually words or discrete terms).\n",
    "- **Normalization** is the name we give most other text pre-processing tasks, including stemming, lemmatization, upper and lowercasing, and stopwords removal.\n",
    "   \n",
    "   -- **Stemming** is the normalization pre-processing task focused on removing word affixes.\n",
    "   \n",
    "   -- **Lemmatization** is the normalization pre-processing task that more carefully brings words down to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f309d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19126b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b205e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part-of-Speech Tagging Function:\n",
    "def get_part_of_speech(word):\n",
    "    # synonyms matching\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    # Initializing counter class objects\n",
    "    pos_counts = Counter()\n",
    "    # Tagging and counting tags\n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"]) # noun\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos() == \"v\"]) # Verb\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos() == \"a\"]) # Adjectiveif\n",
    "    pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos() == \"r\"]) # Adverb\n",
    "    \n",
    "    # The most common tag, the tag with the highest count, en: n for Noun\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1107699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    cleaned = re.sub(r'\\d+', ' ', cleaned)\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    tokenized_no_stopwords = [word for word in tokenized if word not in stop_words]\n",
    "    normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token))for token in tokenized_no_stopwords ])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9d2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Articles Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from articles import articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17524f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SYDNEY: Cricket fever has gripped Australia with the World Cup just days away. Fans from around the world have thronged to the country and hotels are capitalising. Prices of rooms have almost doubled to 300 dollars and hotels are experiencing full bookings. Experts estimate that during the mega event Australia will generate 1.5 million US dollars just from hotel bookings. If the cost of internal air travel, taxis and tickets is taken into consideration, Australia stands to generate two million US dollars during the World Cup.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# articles sample\n",
    "articles[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992a0f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sydney cricket fever grip australia world cup day away fan around world throng country hotel capitalise price room almost double dollar hotel experience full book expert estimate mega event australia generate million u dollar hotel book cost internal air travel taxi ticket take consideration australia stand generate two million u dollar world cup'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocess the articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "processed_articles[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7af2b",
   "metadata": {},
   "source": [
    "### Calculate tf-idf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcec7e2",
   "metadata": {},
   "source": [
    "### Method 1 : TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045c2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The norm = None keyword argument prevents scikit-learn from modifying the multiplication of term frequency\n",
    "corpus_vectorizer = TfidfVectorizer(norm=None)\n",
    "# fit and transform the training data and returns a score matrix\n",
    "corpus_tfidf_scores = corpus_vectorizer.fit_transform(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607d4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 167)\t2.2992829841302607\n",
      "  (0, 67)\t2.7047480922384253\n",
      "  (0, 61)\t2.7047480922384253\n",
      "  (0, 291)\t2.7047480922384253\n",
      "  (0, 48)\t5.4094961844768505\n",
      "  (0, 113)\t2.7047480922384253\n",
      "  (0, 185)\t2.7047480922384253\n",
      "  (0, 52)\t2.7047480922384253\n",
      "  (0, 242)\t2.7047480922384253\n",
      "  (0, 298)\t2.01160091167848\n",
      "  (0, 197)\t2.7047480922384253\n",
      "  (0, 3)\t2.2992829841302607\n",
      "  (0, 56)\t2.01160091167848\n",
      "  (0, 195)\t2.2992829841302607\n",
      "  (0, 51)\t2.7047480922384253\n",
      "  (0, 165)\t2.7047480922384253\n",
      "  (0, 43)\t2.7047480922384253\n",
      "  (0, 50)\t2.7047480922384253\n",
      "  (0, 35)\t5.4094961844768505\n",
      "  (0, 143)\t2.7047480922384253\n",
      "  (0, 207)\t2.7047480922384253\n",
      "  (0, 65)\t2.7047480922384253\n",
      "  (0, 1)\t2.7047480922384253\n",
      "  (0, 231)\t2.7047480922384253\n",
      "  (0, 157)\t5.4094961844768505\n",
      "  :\t:\n",
      "  (9, 162)\t2.7047480922384253\n",
      "  (9, 13)\t2.7047480922384253\n",
      "  (9, 235)\t2.7047480922384253\n",
      "  (9, 91)\t2.7047480922384253\n",
      "  (9, 105)\t2.7047480922384253\n",
      "  (9, 274)\t2.7047480922384253\n",
      "  (9, 112)\t2.7047480922384253\n",
      "  (9, 312)\t5.4094961844768505\n",
      "  (9, 119)\t2.7047480922384253\n",
      "  (9, 299)\t2.7047480922384253\n",
      "  (9, 21)\t2.7047480922384253\n",
      "  (9, 41)\t2.7047480922384253\n",
      "  (9, 69)\t2.7047480922384253\n",
      "  (9, 54)\t2.7047480922384253\n",
      "  (9, 23)\t2.7047480922384253\n",
      "  (9, 60)\t2.7047480922384253\n",
      "  (9, 24)\t2.7047480922384253\n",
      "  (9, 263)\t2.7047480922384253\n",
      "  (9, 22)\t2.7047480922384253\n",
      "  (9, 313)\t5.4094961844768505\n",
      "  (9, 149)\t5.4094961844768505\n",
      "  (9, 44)\t5.4094961844768505\n",
      "  (9, 27)\t2.7047480922384253\n",
      "  (9, 9)\t2.2992829841302607\n",
      "  (9, 247)\t2.4013413909243027\n"
     ]
    }
   ],
   "source": [
    "print(corpus_tfidf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f354c",
   "metadata": {},
   "source": [
    "### Method 2: Bag-of-Words to tf-idf\n",
    "\n",
    "First create Bag-of-Words model and converted the bag-of-words model into tf-idf scores using scikit-learn's **TfidfTransformer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efb6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "# fit and transform training data and returns a BoW matrix, word count\n",
    "bow_matrix = vectorizer.fit_transform(processed_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9cff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 1</th>\n",
       "      <th>Article 2</th>\n",
       "      <th>Article 3</th>\n",
       "      <th>Article 4</th>\n",
       "      <th>Article 5</th>\n",
       "      <th>Article 6</th>\n",
       "      <th>Article 7</th>\n",
       "      <th>Article 8</th>\n",
       "      <th>Article 9</th>\n",
       "      <th>Article 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbasi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
       "abbasi          0          0          0          1          0          0   \n",
       "abide           1          0          0          0          0          0   \n",
       "accord          0          0          1          0          0          0   \n",
       "add             1          0          0          0          0          0   \n",
       "agency          0          0          0          0          0          0   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "world           0          0          0          0          0          3   \n",
       "would           0          0          0          1          0          0   \n",
       "year            0          1          0          0          0          0   \n",
       "yi              0          0          0          0          0          0   \n",
       "yuan            0          0          0          0          0          0   \n",
       "\n",
       "        Article 7  Article 8  Article 9  Article 10  \n",
       "abbasi          0          0          0           0  \n",
       "abide           0          0          0           0  \n",
       "accord          0          0          0           0  \n",
       "add             0          0          1           0  \n",
       "agency          1          0          0           0  \n",
       "...           ...        ...        ...         ...  \n",
       "world           0          0          0           0  \n",
       "would           0          0          1           0  \n",
       "year            0          0          0           0  \n",
       "yi              0          0          0           2  \n",
       "yuan            0          0          0           2  \n",
       "\n",
       "[314 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stores BoW vectors results in a DataFrame\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# create an articles index\n",
    "articles_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "# create pandas DataFrame with feature names\n",
    "# The .T stands for transpose() and todense() returns a dense matrix representation\n",
    "df_bag_of_words = pd.DataFrame(bow_matrix.T.todense(), index=feature_names, columns=articles_index)\n",
    "df_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5052ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the bag of words to csv file\n",
    "df_bag_of_words.to_csv('data/articles_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3858df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the bag-of-words model to tf-idf\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "# transforms and return scores\n",
    "tfidf_scores = transformer.fit_transform(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16af1c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 1</th>\n",
       "      <th>Article 2</th>\n",
       "      <th>Article 3</th>\n",
       "      <th>Article 4</th>\n",
       "      <th>Article 5</th>\n",
       "      <th>Article 6</th>\n",
       "      <th>Article 7</th>\n",
       "      <th>Article 8</th>\n",
       "      <th>Article 9</th>\n",
       "      <th>Article 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbasi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.114244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
       "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
       "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
       "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "agency   0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
       "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
       "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
       "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "\n",
       "        Article 7  Article 8  Article 9  Article 10  \n",
       "abbasi   0.000000        0.0   0.000000    0.000000  \n",
       "abide    0.000000        0.0   0.000000    0.000000  \n",
       "accord   0.000000        0.0   0.000000    0.000000  \n",
       "add      0.000000        0.0   2.299283    0.000000  \n",
       "agency   2.704748        0.0   0.000000    0.000000  \n",
       "...           ...        ...        ...         ...  \n",
       "world    0.000000        0.0   0.000000    0.000000  \n",
       "would    0.000000        0.0   2.299283    0.000000  \n",
       "year     0.000000        0.0   0.000000    0.000000  \n",
       "yi       0.000000        0.0   0.000000    5.409496  \n",
       "yuan     0.000000        0.0   0.000000    5.409496  \n",
       "\n",
       "[314 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stores the scores results from TfidfTransformer in a DataFrame\n",
    "df_tfidf_scores = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=articles_index)\n",
    "df_tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db0d4838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 1</th>\n",
       "      <th>Article 2</th>\n",
       "      <th>Article 3</th>\n",
       "      <th>Article 4</th>\n",
       "      <th>Article 5</th>\n",
       "      <th>Article 6</th>\n",
       "      <th>Article 7</th>\n",
       "      <th>Article 8</th>\n",
       "      <th>Article 9</th>\n",
       "      <th>Article 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abbasi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.114244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.299283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.704748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.409496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
       "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
       "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
       "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "agency   0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
       "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
       "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
       "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "\n",
       "        Article 7  Article 8  Article 9  Article 10  \n",
       "abbasi   0.000000        0.0   0.000000    0.000000  \n",
       "abide    0.000000        0.0   0.000000    0.000000  \n",
       "accord   0.000000        0.0   0.000000    0.000000  \n",
       "add      0.000000        0.0   2.299283    0.000000  \n",
       "agency   2.704748        0.0   0.000000    0.000000  \n",
       "...           ...        ...        ...         ...  \n",
       "world    0.000000        0.0   0.000000    0.000000  \n",
       "would    0.000000        0.0   2.299283    0.000000  \n",
       "year     0.000000        0.0   0.000000    0.000000  \n",
       "yi       0.000000        0.0   0.000000    5.409496  \n",
       "yuan     0.000000        0.0   0.000000    5.409496  \n",
       "\n",
       "[314 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stores the scores results from TfidfVectorizer in a DataFrame\n",
    "df_corpus_tfidf_scores = pd.DataFrame(corpus_tfidf_scores.T.todense(), index=feature_names, columns=articles_index)\n",
    "df_corpus_tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf71400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus_tfidf_scores to csv file\n",
    "df_corpus_tfidf_scores.to_csv('data/articles_corpus_tfidf_scores.csv')\n",
    "# save the corpus_tfidf_scores to csv file\n",
    "df_tfidf_scores.to_csv('data/articles_tfidf_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2f626",
   "metadata": {},
   "source": [
    "### Scores results Comparison:\n",
    "Confirms that the tf-idf scores given by **TfidfTransformer** and **TfidfVectorizer** are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1af334de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the tf-idf scores the same?: yes\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(corpus_tfidf_scores.todense(),tfidf_scores.todense()):\n",
    "    print(\"Are the tf-idf scores the same?: yes\")\n",
    "else:\n",
    "    print(\"Are the tf-idf scores the same?: NO, something is wrong:(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ea675",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "A simple way of identifying the “topic” of a document is to label the document with its highest-scoring tf-idf term. While this is a more naive approach than others, it is a quick and easy way of getting insight into the topic of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8028a37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Article 1</th>\n",
       "      <td>fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 2</th>\n",
       "      <td>hong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 3</th>\n",
       "      <td>sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 4</th>\n",
       "      <td>petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 5</th>\n",
       "      <td>engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 6</th>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 7</th>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 8</th>\n",
       "      <td>railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 9</th>\n",
       "      <td>cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article 10</th>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic\n",
       "Article 1        fare\n",
       "Article 2        hong\n",
       "Article 3       sugar\n",
       "Article 4      petrol\n",
       "Article 5      engine\n",
       "Article 6   australia\n",
       "Article 7         car\n",
       "Article 8     railway\n",
       "Article 9     cabinet\n",
       "Article 10      china"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# articles highest-scoring tf-idf terms aka topics\n",
    "# create a list of the indexes' highest value (pandas.Series.idsmax())\n",
    "topics = [df_tfidf_scores[f'Article {i+1}'].idxmax() for i in range(len(articles_index))]\n",
    "# create DataFrame\n",
    "df_articles_topics = pd.DataFrame({'Topic':topics}, index=articles_index)\n",
    "df_articles_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce98b20",
   "metadata": {},
   "source": [
    "By using NLP supervised machine learning models we can gain insight into news articles topics without having the need to read them.\n",
    "For example, we can determine, with good certainty, that the article 6's topic is linked to Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d99790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topic of the respective articles into csv file\n",
    "df_articles_topics.to_csv('data/articles_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff413ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
