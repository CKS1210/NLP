{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f865e6ff",
   "metadata": {},
   "source": [
    "We'll use NLTK (Natural Language ToolKit) library here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4daf5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc173ff",
   "metadata": {},
   "source": [
    "### 1. Text Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f64cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather in singapore is just too hot!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "input_str = \"Weather in Singapore is just too Hot!\"\n",
    "lowercase_text(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b766cca",
   "metadata": {},
   "source": [
    "### 2. Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ede3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is first day of !!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_num(text):\n",
    "    result = re.sub(r'\\d+ ', '', text)\n",
    "    return result\n",
    "\n",
    "input_str = \"Today is first day of 2024!!\"\n",
    "remove_num(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c59d2",
   "metadata": {},
   "source": [
    "Another method of converting numbers into words. This could be done by using the inflect library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6874183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today I bought five packets of rice, two packets of biscuit, one full trolly of snacks.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inflect\n",
    "q = inflect.engine()\n",
    "\n",
    "# convert number into text\n",
    "def convert_num(text):\n",
    "    # split strings into list of texts\n",
    "    temp_string = text.split()\n",
    "    # initialize empty list\n",
    "    new_str = []\n",
    "    \n",
    "    for word in temp_string:\n",
    "        if word.isdigit():\n",
    "            temp = q.number_to_words(word)\n",
    "            new_str.append(temp)\n",
    "        else:\n",
    "            new_str.append(word)\n",
    "            \n",
    "    # join the texts of new_str to form a string\n",
    "    temp_str = \" \".join(new_str)\n",
    "    return temp_str\n",
    "\n",
    "\n",
    "input_str = \"Today I bought 5 packets of rice, 2 packets of biscuit, 1 full trolly of snacks.\"\n",
    "convert_num(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b75b3",
   "metadata": {},
   "source": [
    "### 3. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c74b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey are you excited I am really looking forward to go to Japan'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    translator = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str = \"Hey, are you excited??? I am really looking forward to go to Japan!!!\"\n",
    "remove_punct(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de1a54",
   "metadata": {},
   "source": [
    "### 4. Remove Stopwords\n",
    "**`Stopwords`** are words that do not contribute to the meaning of the sentence. Hence, they can safely removed without causing any change in the meaning of a sentence. The NLTK (Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens.\n",
    "\n",
    "Examples of stop words in English are “a,” “the,” “is,” “are,” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db387b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'likes', 'A.I', 'Machine', 'Learning', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "input_str = \"I likes A.I and Machine Learning.\"\n",
    "rem_stopwords(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd25431",
   "metadata": {},
   "source": [
    "### 5. Stemming\n",
    "Stemming is a process of getting the root form of a word. Root of Stem is the part to which inflextional affixes (like -ed, -ize etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, Stemming a word may not result in actual words. \n",
    "\n",
    "**Example:** \n",
    "            \n",
    "            Mangoes --> Mango\n",
    "\n",
    "            Boys    --> Boy \n",
    "\n",
    "            going   --> go\n",
    "\n",
    "If the sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into root form. These are the Porter Stemmer, the Snowball Stemmer, and the Lancaster Stemmer. Usually we use Porter Stemmer among them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600756d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wish', 'everyon', 'ha', 'a', 'great', 'great', 'new', 'year', '!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem1 = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_token = word_tokenize(text)\n",
    "    stems = [stem1.stem(word) for word in word_token]\n",
    "    return stems\n",
    "\n",
    "input_str = \"Wishing everyone has a great great new year!\"\n",
    "stem_words(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f9ae5",
   "metadata": {},
   "source": [
    "### 6. Lemmatization\n",
    "As Stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK (Natural Language Toolkit), we use the Word Lemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization. So, we added pos (parts-of-speech) as a parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33ee9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Wishing', 'everyone', 'have', 'a', 'great', 'great', 'new', 'year', '!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatize string \n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech(pos)\n",
    "    lemmas = [lemma.lemmatize(word, pos='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "input_str = \"Wishing everyone has a great great new year!\"\n",
    "lemmatize_word(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8b979",
   "metadata": {},
   "source": [
    "### 7. Part of Speech (POS)\n",
    "The POS (Parts of Speech) explains you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing (NLP) models like bag-of-words(BOW) fails to identify these relation between the words. For that we use POS tagging to mark a word to its POS tag based on its context in the data. POS is also used to extract relationship between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f27685cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Wishing', 'VBG'),\n",
       " ('everyone', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('great', 'JJ'),\n",
       " ('new', 'JJ'),\n",
       " ('year', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_taggg(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "input_str = \"Wishing everyone has a great great new year!\"\n",
    "pos_taggg(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c3a9e1",
   "metadata": {},
   "source": [
    "* VBG --> verb gerund (judging)\n",
    "* NN --> Noun\n",
    "* VBZ --> Verb, present tense not 3rd person singular (wrap)\n",
    "* DT --> Determiner\n",
    "* JJ --> Adjective(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e93227bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "\n",
    "# extract information about the tag\n",
    "nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58e295e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# extract information about the tag\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263614b5",
   "metadata": {},
   "source": [
    "### 8. Chuncking\n",
    "Chunking is the porcess of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing. We can do it on top of pos tagging. It groups words into chuncks mainly for noun phrases. Chunking we do by using regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94ef1831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP The/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def chuncking(text, grammar):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # label words with pos\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    # create chunk parser using grammar\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    # test it on the list of word tokens with tagges pos\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "        \n",
    "sentence = \"The little red parrot is flying in the sky\"\n",
    "grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chuncking(sentence,grammar )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76663fc",
   "metadata": {},
   "source": [
    "In the example above, we defined the grammar by using the regular expression rule. This rule tells you that NP (Noun Phrase) chunk should be formed whenever the chuncker find the optional **determiner (DJ)** followed by any **no. of adjectives** and then a **Noun (NN)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96900290",
   "metadata": {},
   "source": [
    "### 9. Named Entity Recognition (NER)\n",
    "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba58a183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  (GPE England/NNP)\n",
      "  !/.\n",
      "  !/.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\6917\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def ner(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # pos-tagging of words\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    # tree of word entities\n",
    "    print(ne_chunk(word_pos))\n",
    "    \n",
    "    \n",
    "input_str = \"Brain Lara scored the highest 400 runs in a test match which played in between WI and England!!\"\n",
    "ner(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5285",
   "metadata": {},
   "source": [
    "### 10. Understand Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494563c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cfa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead9085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48973689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
